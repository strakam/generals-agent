Variance Reduction:
- (return/advantage) normalization ?
- one value function that sees everything?
- moving average instead of value function? Or average over minibatch?
- Add KL divergence to loss? -> need to calculate it correctly because of more complex action space

- Reward shaping - is it dangerous or lez go?
- what should i optimize first? hyper-params? scaling on maybe 2 gpus? should i try smaller network?

- clip timesteps on 700
- make actions end-to-end without conditioning -> + increased confidence that PPO is correct, - much larger action space (is it?)
- selfplay simultaneously or interleave



TODOs:
- checkpointing
- eval vs OG
- add shared memory for async env? 
- try moving env.reset() outside
