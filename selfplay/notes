Variance Reduction:
- (return/advantage) normalization ?
- one oracle value function that sees everything?
- Add KL divergence to loss? -> need to calculate it correctly because of more complex action space
- Reward shaping - is it dangerous or lez go?


For selfplay
- set good n_steps value (700 sounds nice)
- make actions end-to-end without conditioning -> + increased confidence that PPO is correct, - much larger action space (is it?) - probably not a good idea
- selfplay simultaneously or interleave ?
- probably need for big compute -> focus a bit on smaller network? (reduction by 10M params seems to work)
- it took a lot of time (?) to train against random policy


- How to scale?
- focus on more gpus and envs?
- get working critic? smaller base network?
- reward shaping?
- something completely different? like temperature when sampling actions or e-greedy


TODOs:
- checkpointing
- eval vs OG
- add shared memory for async env? -- tried, looks like yes
- try moving env.reset() outside
- revert normalization as now i use experimental one
